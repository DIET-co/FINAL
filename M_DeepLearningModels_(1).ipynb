{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "OQfTXN6kYw2z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQfTXN6kYw2z",
        "outputId": "705b4529-91ad-4ece-e61d-b94363c18048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27ed6f00",
      "metadata": {
        "id": "27ed6f00"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import json\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = r'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Use only a small portion of the dataset for demonstration\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the texts\n",
        "max_words = 1000000  # Adjust based on your dataset size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Save the tokenizer here\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 50  # Adjust based on your dataset\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the model\n",
        "embedding_dim = 16\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32  # Adjust based on your resources\n",
        "epochs = 10  # Adjust based on your requirements\n",
        "model.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), batch_size=batch_size, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b437cb0",
      "metadata": {
        "id": "2b437cb0"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/FINAL/pichi/CNN_MODEL_OPTIMIZED.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb6d8c83",
      "metadata": {
        "id": "eb6d8c83"
      },
      "outputs": [],
      "source": [
        "# FNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ebc5d07",
      "metadata": {
        "id": "0ebc5d07"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = r'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the texts\n",
        "max_words = 1000000  # Adjust based on your dataset size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 50  # Adjust based on your dataset\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the Feedforward Neural Network (FNN) model\n",
        "embedding_dim = 16\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32  # Adjust based on your resources\n",
        "epochs = 10  # Adjust based on your requirements\n",
        "model.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), batch_size=batch_size, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "287b53a3",
      "metadata": {
        "id": "287b53a3"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/FINAL/pichi/FNN_MODEL_OPTIMISED.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dd856dc",
      "metadata": {
        "id": "6dd856dc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = r'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the texts\n",
        "max_words = 1000000  # Adjust based on your dataset size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 50  # Adjust based on your dataset\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the LSTM model\n",
        "embedding_dim = 16\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model_lstm.add(Bidirectional(LSTM(64)))\n",
        "model_lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32  # Adjust based on your resources\n",
        "epochs = 10  # Adjust based on your requirements\n",
        "model_lstm.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), batch_size=batch_size, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a817c4f",
      "metadata": {
        "id": "4a817c4f"
      },
      "outputs": [],
      "source": [
        "x=df['target']\n",
        "x\n",
        "z=0\n",
        "for i in x:\n",
        "    if i==1:\n",
        "        z=z+1\n",
        "\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31e4bcb0",
      "metadata": {
        "id": "31e4bcb0"
      },
      "outputs": [],
      "source": [
        "model_lstm.save('/content/drive/MyDrive/FINAL/pichi/LSTM_MODEL_OPTIMISED.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fe013c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fe013c2",
        "outputId": "9d941a95-1d6c-4413-b20a-9a42356e3e16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "40000/40000 [==============================] - 391s 10ms/step - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.5016\n",
            "Epoch 2/5\n",
            "40000/40000 [==============================] - 378s 9ms/step - loss: 0.6932 - accuracy: 0.4994 - val_loss: 0.6933 - val_accuracy: 0.4984\n",
            "Epoch 3/5\n",
            "40000/40000 [==============================] - 361s 9ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.4984\n",
            "Epoch 4/5\n",
            "40000/40000 [==============================] - 377s 9ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.5016\n",
            "Epoch 5/5\n",
            "40000/40000 [==============================] - 360s 9ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6932 - val_accuracy: 0.4984\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fbe1815b910>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = r'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the texts\n",
        "max_words = 1000000 # Adjust based on your dataset size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 50  # Adjust based on your dataset\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the GRU model\n",
        "embedding_dim = 16\n",
        "model_gru = Sequential()\n",
        "model_gru.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model_gru.add(GRU(64))\n",
        "model_gru.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_gru.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32  # Adjust based on your resources\n",
        "epochs = 10  # Adjust based on your requirements\n",
        "model_gru.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), batch_size=batch_size, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "628f4c18",
      "metadata": {
        "id": "628f4c18"
      },
      "outputs": [],
      "source": [
        "model_gru.save('/content/drive/MyDrive/FINAL/pichi/GRU_MODEL_OPTIMISED.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4dc4b90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4dc4b90",
        "outputId": "29ad7dd8-e38e-4561-ec0a-5eab41fd5ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "40000/40000 [==============================] - 1640s 41ms/step - loss: 0.6423 - accuracy: 0.6268 - val_loss: 0.6540 - val_accuracy: 0.6082\n",
            "Epoch 2/5\n",
            "40000/40000 [==============================] - 1658s 41ms/step - loss: 0.6546 - accuracy: 0.6104 - val_loss: 0.6636 - val_accuracy: 0.6022\n",
            "Epoch 3/5\n",
            "40000/40000 [==============================] - 1629s 41ms/step - loss: 0.6561 - accuracy: 0.6140 - val_loss: 0.6601 - val_accuracy: 0.6019\n",
            "Epoch 4/5\n",
            "40000/40000 [==============================] - 1590s 40ms/step - loss: 0.6591 - accuracy: 0.6009 - val_loss: 0.6622 - val_accuracy: 0.5933\n",
            "Epoch 5/5\n",
            "40000/40000 [==============================] - 1619s 40ms/step - loss: 0.6681 - accuracy: 0.5681 - val_loss: 0.6749 - val_accuracy: 0.5586\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fbe10186830>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = r'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the texts\n",
        "max_words = 1000000  # Adjust based on your dataset size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 50  # Adjust based on your dataset\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the RNN model\n",
        "embedding_dim = 16\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model_rnn.add(SimpleRNN(64))\n",
        "model_rnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_rnn.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32  # Adjust based on your resources\n",
        "epochs = 10  # Adjust based on your requirements\n",
        "model_rnn.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), batch_size=batch_size, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8590a8aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8590a8aa",
        "outputId": "f66d8a01-b517-475e-ce96-6cd6216a55ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model_rnn.save('/content/drive/MyDrive/FINAL/RNN_MODEL_OPTIMISED.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c74ec59d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "c74ec59d",
        "outputId": "9a1a4cbd-57ec-48e5-9e2b-dc8ce476b76f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bb4e26505010>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load the preprocessed dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = r'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the texts\n",
        "max_words = 1000000  # Adjust based on your dataset size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 50  # Adjust based on your dataset\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the Bidirectional RNN model\n",
        "embedding_dim = 16\n",
        "model_bidirectional_rnn = Sequential()\n",
        "model_bidirectional_rnn.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model_bidirectional_rnn.add(Bidirectional(SimpleRNN(64)))\n",
        "model_bidirectional_rnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_bidirectional_rnn.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32  # Adjust based on your resources\n",
        "epochs = 10  # Adjust based on your requirements\n",
        "model_bidirectional_rnn.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), batch_size=batch_size, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ee703b6",
      "metadata": {
        "id": "7ee703b6"
      },
      "outputs": [],
      "source": [
        "model_bidirectional_rnn.save('/content/drive/MyDrive/FINAL/BI_RNN_MODEL_OPTIMISED.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca706cc1",
      "metadata": {
        "id": "ca706cc1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = r'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'text' column to strings\n",
        "df['text'] = df['text'].astype(str)\n",
        "\n",
        "# Use only a small portion of the dataset for demonstration\n",
        "df = shuffle(df)\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the texts\n",
        "max_words = 1000000  # Adjust based on your dataset size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 50  # Adjust based on your dataset\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the model\n",
        "embedding_dim = 16\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32  # Adjust based on your resources\n",
        "epochs = 10  # Adjust based on your requirements\n",
        "model.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), batch_size=batch_size, epochs=epochs)\n",
        "model.save('/content/drive/MyDrive/FINAL/CNN_POWERFUL.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34e879e1",
      "metadata": {
        "id": "34e879e1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}