{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQfTXN6kYw2z",
        "outputId": "00c4cf42-9744-4617-c499-84754f28d311"
      },
      "id": "OQfTXN6kYw2z",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "27ed6f00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27ed6f00",
        "outputId": "12b8ef66-167f-4a3e-af4d-821d161d0206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "125/125 [==============================] - 11s 58ms/step - loss: 0.6848 - accuracy: 0.5518 - val_loss: 0.6566 - val_accuracy: 0.6250\n",
            "Epoch 2/5\n",
            "125/125 [==============================] - 5s 42ms/step - loss: 0.5300 - accuracy: 0.7550 - val_loss: 0.5851 - val_accuracy: 0.7030\n",
            "Epoch 3/5\n",
            "125/125 [==============================] - 7s 54ms/step - loss: 0.3203 - accuracy: 0.8798 - val_loss: 0.6248 - val_accuracy: 0.7000\n",
            "Epoch 4/5\n",
            "125/125 [==============================] - 5s 42ms/step - loss: 0.1742 - accuracy: 0.9433 - val_loss: 0.7593 - val_accuracy: 0.6960\n",
            "Epoch 5/5\n",
            "125/125 [==============================] - 7s 56ms/step - loss: 0.0983 - accuracy: 0.9705 - val_loss: 0.8306 - val_accuracy: 0.6730\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x799752079240>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = r'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Use only a small portion of the dataset for demonstration\n",
        "df = shuffle(df)\n",
        "df = df.head(5000)  # Adjust the number based on your available resources\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the texts\n",
        "max_words = 10000  # Adjust based on your dataset size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 50  # Adjust based on your dataset\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the model\n",
        "embedding_dim = 16\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32  # Adjust based on your resources\n",
        "epochs = 5  # Adjust based on your requirements\n",
        "model.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), batch_size=batch_size, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2b437cb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b437cb0",
        "outputId": "a1d54080-185c-4c82-daf9-59b9c18e3c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('CNN_MODEL_OPTIMIZED.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "eb6d8c83",
      "metadata": {
        "id": "eb6d8c83"
      },
      "outputs": [],
      "source": [
        "# FNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0ebc5d07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ebc5d07",
        "outputId": "3962db24-d133-41cb-b0ea-b8cb865d05cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "125/125 [==============================] - 2s 7ms/step - loss: 0.6905 - accuracy: 0.5307 - val_loss: 0.6822 - val_accuracy: 0.5810\n",
            "Epoch 2/5\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.5500 - accuracy: 0.7605 - val_loss: 0.6062 - val_accuracy: 0.6800\n",
            "Epoch 3/5\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.1878 - accuracy: 0.9490 - val_loss: 0.7880 - val_accuracy: 0.6510\n",
            "Epoch 4/5\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0420 - accuracy: 0.9933 - val_loss: 0.8367 - val_accuracy: 0.6620\n",
            "Epoch 5/5\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0147 - accuracy: 0.9992 - val_loss: 1.0603 - val_accuracy: 0.6430\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79975cab1510>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = r'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Use only a small portion of the dataset for demonstration\n",
        "df = shuffle(df)\n",
        "df = df.head(5000)  # Adjust the number based on your available resources\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the texts\n",
        "max_words = 10000  # Adjust based on your dataset size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 50  # Adjust based on your dataset\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the Feedforward Neural Network (FNN) model\n",
        "embedding_dim = 16\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32  # Adjust based on your resources\n",
        "epochs = 5  # Adjust based on your requirements\n",
        "model.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), batch_size=batch_size, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "287b53a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "287b53a3",
        "outputId": "064d1469-722f-4f37-dd7b-ae498f2866c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('FNN_MODEL_OPTIMISED.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6dd856dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dd856dc",
        "outputId": "a8a838f8-1224-4839-948f-900a4882a8d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "125/125 [==============================] - 11s 63ms/step - loss: 0.6785 - accuracy: 0.5835 - val_loss: 0.6213 - val_accuracy: 0.6840\n",
            "Epoch 2/5\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 0.5018 - accuracy: 0.7793 - val_loss: 0.5563 - val_accuracy: 0.7280\n",
            "Epoch 3/5\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 0.2944 - accuracy: 0.8903 - val_loss: 0.6284 - val_accuracy: 0.6970\n",
            "Epoch 4/5\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 0.1628 - accuracy: 0.9495 - val_loss: 0.7874 - val_accuracy: 0.6980\n",
            "Epoch 5/5\n",
            "125/125 [==============================] - 5s 43ms/step - loss: 0.0894 - accuracy: 0.9720 - val_loss: 0.8923 - val_accuracy: 0.6720\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79975202bdf0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = r'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Use only a small portion of the dataset for demonstration\n",
        "df = shuffle(df)\n",
        "df = df.head(5000)  # Adjust the number based on your available resources\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the texts\n",
        "max_words = 10000  # Adjust based on your dataset size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 50  # Adjust based on your dataset\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the LSTM model\n",
        "embedding_dim = 16\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model_lstm.add(Bidirectional(LSTM(64)))\n",
        "model_lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32  # Adjust based on your resources\n",
        "epochs = 5  # Adjust based on your requirements\n",
        "model_lstm.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), batch_size=batch_size, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4a817c4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a817c4f",
        "outputId": "fd0ae961-b1dc-4261-ec83-98a6a041ee6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2538"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "x=df['target']\n",
        "x\n",
        "z=0\n",
        "for i in x:\n",
        "    if i==1:\n",
        "        z=z+1\n",
        "\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "31e4bcb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31e4bcb0",
        "outputId": "9c79ccac-9569-4f0e-9708-f930f21bf4ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('LSTM_MODEL_OPTIMISED.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3fe013c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fe013c2",
        "outputId": "2eef6bd6-453a-4c83-f281-e5d532ecdc3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "125/125 [==============================] - 5s 28ms/step - loss: 0.6934 - accuracy: 0.5095 - val_loss: 0.6930 - val_accuracy: 0.5110\n",
            "Epoch 2/5\n",
            "125/125 [==============================] - 3s 28ms/step - loss: 0.6931 - accuracy: 0.5130 - val_loss: 0.6929 - val_accuracy: 0.5110\n",
            "Epoch 3/5\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.6933 - accuracy: 0.5130 - val_loss: 0.6929 - val_accuracy: 0.5110\n",
            "Epoch 4/5\n",
            "125/125 [==============================] - 3s 24ms/step - loss: 0.6930 - accuracy: 0.5130 - val_loss: 0.6930 - val_accuracy: 0.5110\n",
            "Epoch 5/5\n",
            "125/125 [==============================] - 3s 24ms/step - loss: 0.6931 - accuracy: 0.5130 - val_loss: 0.6929 - val_accuracy: 0.5110\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x799737d1bfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = r'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Use only a small portion of the dataset for demonstration\n",
        "df = shuffle(df)\n",
        "df = df.head(5000)  # Adjust the number based on your available resources\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the texts\n",
        "max_words = 10000  # Adjust based on your dataset size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 50  # Adjust based on your dataset\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the GRU model\n",
        "embedding_dim = 16\n",
        "model_gru = Sequential()\n",
        "model_gru.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model_gru.add(GRU(64))\n",
        "model_gru.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_gru.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32  # Adjust based on your resources\n",
        "epochs = 5  # Adjust based on your requirements\n",
        "model_gru.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), batch_size=batch_size, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "628f4c18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "628f4c18",
        "outputId": "60823e5e-121a-4694-8023-ca11fb41ff47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('GRU_MODEL_OPTIMISED.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f4dc4b90",
      "metadata": {
        "id": "f4dc4b90",
        "outputId": "00c0f838-28c7-4fd7-b2f1-9a5cad224920",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "125/125 [==============================] - 4s 18ms/step - loss: 0.6948 - accuracy: 0.5065 - val_loss: 0.6940 - val_accuracy: 0.5060\n",
            "Epoch 2/5\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6669 - accuracy: 0.6212 - val_loss: 0.7125 - val_accuracy: 0.5230\n",
            "Epoch 3/5\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.5841 - accuracy: 0.7117 - val_loss: 0.8318 - val_accuracy: 0.5170\n",
            "Epoch 4/5\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.4268 - accuracy: 0.8282 - val_loss: 1.0035 - val_accuracy: 0.5180\n",
            "Epoch 5/5\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.3034 - accuracy: 0.8925 - val_loss: 1.0566 - val_accuracy: 0.5550\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7997399878b0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = r'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Use only a small portion of the dataset for demonstration\n",
        "df = shuffle(df)\n",
        "df = df.head(5000)  # Adjust the number based on your available resources\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the texts\n",
        "max_words = 10000  # Adjust based on your dataset size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 50  # Adjust based on your dataset\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the RNN model\n",
        "embedding_dim = 16\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model_rnn.add(SimpleRNN(64))\n",
        "model_rnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_rnn.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32  # Adjust based on your resources\n",
        "epochs = 5  # Adjust based on your requirements\n",
        "model_rnn.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), batch_size=batch_size, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8590a8aa",
      "metadata": {
        "id": "8590a8aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9a5e54b-3f08-479b-9a55-8588630912ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('RNN_MODEL_OPTIMISED.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c74ec59d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c74ec59d",
        "outputId": "46094934-91d4-4d94-a3b8-6defa27cdede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "125/125 [==============================] - 5s 25ms/step - loss: 0.6926 - accuracy: 0.5255 - val_loss: 0.7020 - val_accuracy: 0.4870\n",
            "Epoch 2/5\n",
            "125/125 [==============================] - 3s 28ms/step - loss: 0.6212 - accuracy: 0.6877 - val_loss: 0.6740 - val_accuracy: 0.5900\n",
            "Epoch 3/5\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 0.3469 - accuracy: 0.8775 - val_loss: 0.7454 - val_accuracy: 0.5930\n",
            "Epoch 4/5\n",
            "125/125 [==============================] - 3s 22ms/step - loss: 0.1052 - accuracy: 0.9747 - val_loss: 1.0108 - val_accuracy: 0.5770\n",
            "Epoch 5/5\n",
            "125/125 [==============================] - 3s 25ms/step - loss: 0.0621 - accuracy: 0.9870 - val_loss: 1.0561 - val_accuracy: 0.5760\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x799752ec5de0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = r'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Use only a small portion of the dataset for demonstration\n",
        "df = shuffle(df)\n",
        "df = df.head(5000)  # Adjust the number based on your available resources\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the texts\n",
        "max_words = 10000  # Adjust based on your dataset size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 50  # Adjust based on your dataset\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the Bidirectional RNN model\n",
        "embedding_dim = 16\n",
        "model_bidirectional_rnn = Sequential()\n",
        "model_bidirectional_rnn.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model_bidirectional_rnn.add(Bidirectional(SimpleRNN(64)))\n",
        "model_bidirectional_rnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_bidirectional_rnn.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32  # Adjust based on your resources\n",
        "epochs = 5  # Adjust based on your requirements\n",
        "model_bidirectional_rnn.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), batch_size=batch_size, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7ee703b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ee703b6",
        "outputId": "88e275b2-661d-419b-bde8-491992cfc960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('BI_RNN_MODEL_OPTIMISED.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca706cc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca706cc1",
        "outputId": "75a41d0e-c294-4c42-f53f-516cc8dbc824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            " 2951/40000 [=>............................] - ETA: 27:52 - loss: 0.4919 - accuracy: 0.7629"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = r'/content/drive/MyDrive/FINAL/preprocessed_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'text' column to strings\n",
        "df['text'] = df['text'].astype(str)\n",
        "\n",
        "# Use only a small portion of the dataset for demonstration\n",
        "df = shuffle(df)\n",
        "\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the texts\n",
        "max_words = 10000  # Adjust based on your dataset size\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 50  # Adjust based on your dataset\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the model\n",
        "embedding_dim = 16\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32  # Adjust based on your resources\n",
        "epochs = 5  # Adjust based on your requirements\n",
        "model.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), batch_size=batch_size, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34e879e1",
      "metadata": {
        "id": "34e879e1"
      },
      "outputs": [],
      "source": [
        "model.save('CNN_POWERFUL.h5')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}